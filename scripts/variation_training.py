# -*- coding: utf-8 -*-
"""variation_training_complete.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1soRAbTtfWWJh7cjl0jJJOA5Lr_I0IYWL
"""

!pip install -U transformers datasets

import transformers
print("Transformers version:", transformers.__version__)

from google.colab import files
uploaded = files.upload()

from datasets import Dataset
import random

with open("variation_ordered.txt", "r", encoding="utf-8") as f:
    lines = [line.strip() for line in f if line.strip()]

random.shuffle(lines)
split1 = int(0.9 * len(lines))
split2 = int(0.95 * len(lines))
train_lines = lines[:split1]
val_lines = lines[split1:split2]
test_lines = lines[split2:]

train_dataset = Dataset.from_dict({"text": train_lines})
val_dataset = Dataset.from_dict({"text": val_lines})
test_dataset = Dataset.from_dict({"text": test_lines})

from transformers import GPT2LMHeadModel, GPT2TokenizerFast

tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.pad_token_id

def tokenize_function(example):
    encoding = tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )
    encoding["labels"] = encoding["input_ids"]
    return encoding

tokenized_train = train_dataset.map(tokenize_function, remove_columns=["text"])
tokenized_val = val_dataset.map(tokenize_function, remove_columns=["text"])

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./results-variation",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=100,
    save_strategy="epoch",
    report_to="none",
    fp16=True
)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
)

trainer.train()

trainer.save_model("checkpoint-variation")

import math
eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")

!zip -r checkpoint-variation.zip checkpoint-variation
from google.colab import files
files.download("checkpoint-variation.zip")