# -*- coding: utf-8 -*-
"""curriculum_gpt2_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pawJoaDYrPkTUx-UFCGl0U5fiRaUYQLR
"""

!pip install -U transformers datasets

from transformers import TrainingArguments, Trainer

!pip install transformers datasets

from google.colab import files
uploaded = files.upload()

from datasets import Dataset
import random

with open("small_ordered.txt", "r", encoding="utf-8") as f:
    lines = [line.strip() for line in f if line.strip()]

random.shuffle(lines)
split1 = int(0.9 * len(lines))
split2 = int(0.95 * len(lines))
train_lines = lines[:split1]
val_lines = lines[split1:split2]
test_lines = lines[split2:]

train_dataset = Dataset.from_dict({"text": train_lines})
val_dataset = Dataset.from_dict({"text": val_lines})
test_dataset = Dataset.from_dict({"text": test_lines})

from transformers import GPT2LMHeadModel, GPT2TokenizerFast

tokenizer = GPT2TokenizerFast.from_pretrained("gpt2", use_auth_token=False)
model = GPT2LMHeadModel.from_pretrained("gpt2", use_auth_token=False)

tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.pad_token_id

def tokenize_function(example):
    encoding = tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )
    encoding["labels"] = encoding["input_ids"]
    return encoding

tokenized_train = train_dataset.map(tokenize_function, remove_columns=["text"])
tokenized_val = val_dataset.map(tokenize_function, remove_columns=["text"])

training_args = TrainingArguments(
    output_dir="./results-curriculum",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=100,
    fp16=True,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
)

trainer.train()

trainer.save_model("checkpoint-curriculum")

import math
eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")

!zip -r checkpoint-curriculum.zip checkpoint-curriculum
from google.colab import files
files.download("checkpoint-curriculum.zip")