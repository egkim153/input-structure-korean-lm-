# Investigating the Effects of Input Structure on Korean Grammar Learning

This repository accompanies the BUCLD abstract titled:  
**"Investigating the Effects of Input Structure on Korean Grammar Learning in Language Models."**

---

## ğŸ“š Project Overview

We explore whether structured inputâ€”such as curriculum learning or translated child-directed speechâ€”can improve the ability of language models to learn Korean grammar.  
The model used is GPT-2 (KoGPT2), fine-tuned under four training conditions:

- `original.txt`: randomly ordered input
- `curriculum.txt`: simple-to-complex grammatical ordering
- `variation.txt`: variation sets with repeated forms
- `translated.txt`: translated child-directed speech

---

## ğŸ“ Repository Structure

```
input-structure-korean-lm/
â”œâ”€â”€ data/                  # Preprocessed training data (txt)
â”œâ”€â”€ scripts/               # Training scripts (.py)
â”œâ”€â”€ notebooks/             # Original Jupyter notebooks (.ipynb)
â”œâ”€â”€ environment.yml        # Conda environment for reproducibility
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ How to Run

### 1. Create environment
```bash
conda env create -f environment.yml
conda activate korean-lm
```

### 2. Run training
```bash
python scripts/original_training.py
python scripts/curriculum_training.py
python scripts/variation_training.py
python scripts/translated_training.py
```

---

## ğŸ“Š Output

Each model was trained on 10,000 sentences for 3 epochs.  
Evaluation is done using **perplexity** as a proxy for grammar acquisition performance.

---

## ğŸ“ Citation

- Haga et al. (2024). *BabyLM Challenge: Exploring the Effect of Variation Sets on Language Model Training Efficiency.*
- Feng et al. (2024). *Is Child-Directed Speech Effective Training Data for Language Models?*
- Kim & Koo (2024). *Evaluating the Impact of Training Conditions on the Performance of GPT-2-Small Based Korean-English Bilingual Models.*
